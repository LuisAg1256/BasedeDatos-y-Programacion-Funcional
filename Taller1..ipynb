{
  "metadata": {
    "name": "Taller1",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val NUM_SAMPLES \u003d 10000000\r\nval count \u003d sc.parallelize(1 to NUM_SAMPLES).filter \r\n{_ \u003d\u003e\r\n\tval x \u003d math.random\r\n\tval y \u003d math.random\r\n\tx*x + y*y \u003c1\r\n}.count()\r\nprint(\"Pi is roughly %f\\n\", 4.0 * count /NUM_SAMPLES)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Hola, soy Luis Aguilar\n\nEsta es mi foto:\n\n![Mi foto](https://i.imgur.com/GdsYHeh.jpeg)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val data \u003d spark\r\n\t.read\r\n\t.option(\"inferSchema\",\"true\")\r\n\t.option(\"header\",\"true\")\r\n\t.option(\"delimiter\",\",\")\r\n\t.csv(\"/workspace/LinuxOnGitpod/Dataset/owid-covid-data.csv\")"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "system.out.print(\"hola\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\r\necho \"Intérpretes instalados:\"\r\nls /path/to/zeppelin/interpreters/\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import avg\n\nspark \u003d SparkSession.builder.appName(\"example\").getOrCreate()\n\ndata_df \u003d spark.createDataFrame([\n    (\"Brooke\", 20),\n    (\"Denny\", 31),\n    (\"Jules\", 30),\n    (\"td\", 35),\n    (\"Brooke\", 25)\n], [\"name\", \"age\"])\n\navg_df \u003d data_df.groupBy(\"name\").agg(avg(\"age\").alias(\"avg_age\"))\navg_df.show()\n\n\n\n"
    }
  ]
}